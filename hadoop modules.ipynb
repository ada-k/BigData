{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark \n",
    "# Local Spark\n",
    "findspark.init('/home/ada/anaconda3/lib/python3.8/site-packages/pyspark') \n",
    "\n",
    "# Cloudera cluster Spark\n",
    "# findspark.init(spark_home='/opt/cloudera/parcels/SPARK2-2.3.0.cloudera4-1.cdh5.13.3.p0.611179/lib/spark2/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get pyspark shell sessions\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName('example_app').master('local[*]').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|namespace|\n",
      "+---------+\n",
      "|  default|\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# display tables in db\n",
    "spark.sql(\"show databases\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+-----+---+---+------+---+-----+-------+---+-----+\n",
      "|_c0|price|speed| hd|ram|screen| cd|multi|premium|ads|trend|\n",
      "+---+-----+-----+---+---+------+---+-----+-------+---+-----+\n",
      "|  1| 1499|   25| 80|  4|    14| no|   no|    yes| 94|    1|\n",
      "|  2| 1795|   33| 85|  2|    14| no|   no|    yes| 94|    1|\n",
      "|  3| 1595|   25|170|  4|    15| no|   no|    yes| 94|    1|\n",
      "|  4| 1849|   25|170|  8|    14| no|   no|     no| 94|    1|\n",
      "|  5| 3295|   33|340| 16|    14| no|   no|    yes| 94|    1|\n",
      "+---+-----+-----+---+---+------+---+-----+-------+---+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# reading dataset using spark\n",
    "df = spark.read.option(\"header\",True).format(\"csv\").load('/home/ada/Downloads/dataset_price_personal_computers.csv')\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+-----------------+------------------+------------------+-----------------+------------------+----+-----+-------+-----------------+-----------------+\n",
      "|summary|               _c0|            price|             speed|                hd|              ram|            screen|  cd|multi|premium|              ads|            trend|\n",
      "+-------+------------------+-----------------+------------------+------------------+-----------------+------------------+----+-----+-------+-----------------+-----------------+\n",
      "|  count|              6259|             6259|              6259|              6259|             6259|              6259|6259| 6259|   6259|             6259|             6259|\n",
      "|   mean|            3130.0|2219.576609682058|52.011024125259624|416.60169356127176|8.286946796612877|14.608723438248921|null| null|   null|221.3010065505672|15.92698514139639|\n",
      "| stddev|1806.9619992314908|580.8039556527061|21.157735384308484| 258.5484451731357|5.631098924402045|0.9051152264048615|null| null|   null|74.83528402648145|7.873983847954109|\n",
      "|    min|                 1|             1049|               100|               100|               16|                14|  no|   no|     no|              100|                1|\n",
      "|    max|               999|              999|                75|               850|                8|                17| yes|  yes|    yes|               95|                9|\n",
      "+-------+------------------+-----------------+------------------+------------------+-----------------+------------------+----+-----+-------+-----------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>price</th>\n",
       "      <th>speed</th>\n",
       "      <th>hd</th>\n",
       "      <th>ram</th>\n",
       "      <th>screen</th>\n",
       "      <th>cd</th>\n",
       "      <th>multi</th>\n",
       "      <th>premium</th>\n",
       "      <th>ads</th>\n",
       "      <th>trend</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1499</td>\n",
       "      <td>25</td>\n",
       "      <td>80</td>\n",
       "      <td>4</td>\n",
       "      <td>14</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>yes</td>\n",
       "      <td>94</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1795</td>\n",
       "      <td>33</td>\n",
       "      <td>85</td>\n",
       "      <td>2</td>\n",
       "      <td>14</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>yes</td>\n",
       "      <td>94</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1595</td>\n",
       "      <td>25</td>\n",
       "      <td>170</td>\n",
       "      <td>4</td>\n",
       "      <td>15</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>yes</td>\n",
       "      <td>94</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1849</td>\n",
       "      <td>25</td>\n",
       "      <td>170</td>\n",
       "      <td>8</td>\n",
       "      <td>14</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>94</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>3295</td>\n",
       "      <td>33</td>\n",
       "      <td>340</td>\n",
       "      <td>16</td>\n",
       "      <td>14</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>yes</td>\n",
       "      <td>94</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  price  speed   hd  ram  screen  cd multi premium  ads  trend\n",
       "0           1   1499     25   80    4      14  no    no     yes   94      1\n",
       "1           2   1795     33   85    2      14  no    no     yes   94      1\n",
       "2           3   1595     25  170    4      15  no    no     yes   94      1\n",
       "3           4   1849     25  170    8      14  no    no      no   94      1\n",
       "4           5   3295     33  340   16      14  no    no     yes   94      1"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from pandas to spark (saving table in spark)\n",
    "machines = pd.read_csv('/home/ada/Downloads/dataset_price_personal_computers.csv')\n",
    "machines.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('index', 'bigint'),\n",
       " ('Unnamed: 0', 'bigint'),\n",
       " ('price', 'bigint'),\n",
       " ('speed', 'bigint'),\n",
       " ('hd', 'bigint'),\n",
       " ('ram', 'bigint'),\n",
       " ('screen', 'bigint'),\n",
       " ('cd', 'string'),\n",
       " ('multi', 'string'),\n",
       " ('premium', 'string'),\n",
       " ('ads', 'bigint'),\n",
       " ('trend', 'bigint')]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "machines.reset_index(inplace=True)\n",
    "machines_spark = spark.createDataFrame(machines)\n",
    "machines_spark.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+\n",
      "|speed|price|\n",
      "+-----+-----+\n",
      "|   25| 1499|\n",
      "|   33| 1795|\n",
      "|   25| 1595|\n",
      "|   25| 1849|\n",
      "|   33| 3295|\n",
      "+-----+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "machines_spark.select('speed', 'price').show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bits of analysis with spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6259"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "machines_spark.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- index: long (nullable = true)\n",
      " |-- Unnamed: 0: long (nullable = true)\n",
      " |-- price: long (nullable = true)\n",
      " |-- speed: long (nullable = true)\n",
      " |-- hd: long (nullable = true)\n",
      " |-- ram: long (nullable = true)\n",
      " |-- screen: long (nullable = true)\n",
      " |-- cd: string (nullable = true)\n",
      " |-- multi: string (nullable = true)\n",
      " |-- premium: string (nullable = true)\n",
      " |-- ads: long (nullable = true)\n",
      " |-- trend: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# column details\n",
    "machines_spark.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+-----+-----+---+---+------+---+-----+-------+---+-----+\n",
      "|index|Unnamed: 0|price|speed| hd|ram|screen| cd|multi|premium|ads|trend|\n",
      "+-----+----------+-----+-----+---+---+------+---+-----+-------+---+-----+\n",
      "|    0|         1| 1499|   25| 80|  4|    14| no|   no|    yes| 94|    1|\n",
      "|    1|         2| 1795|   33| 85|  2|    14| no|   no|    yes| 94|    1|\n",
      "|    2|         3| 1595|   25|170|  4|    15| no|   no|    yes| 94|    1|\n",
      "|    4|         5| 3295|   33|340| 16|    14| no|   no|    yes| 94|    1|\n",
      "|    5|         6| 3695|   66|340| 16|    14| no|   no|    yes| 94|    1|\n",
      "+-----+----------+-----+-----+---+---+------+---+-----+-------+---+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# querying\n",
    "machines_spark.filter(machines_spark[\"premium\"] == \"yes\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+------------------+-----------------+------------------+------------------+-----------------+------------------+----+-----+-------+-----------------+-----------------+\n",
      "|summary|             index|        Unnamed: 0|            price|             speed|                hd|              ram|            screen|  cd|multi|premium|              ads|            trend|\n",
      "+-------+------------------+------------------+-----------------+------------------+------------------+-----------------+------------------+----+-----+-------+-----------------+-----------------+\n",
      "|  count|              6259|              6259|             6259|              6259|              6259|             6259|              6259|6259| 6259|   6259|             6259|             6259|\n",
      "|   mean|            3129.0|            3130.0|2219.576609682058|52.011024125259624|416.60169356127176|8.286946796612877|14.608723438248921|null| null|   null|221.3010065505672|15.92698514139639|\n",
      "| stddev|1806.9619992314908|1806.9619992314908|580.8039556527063|  21.1577353843085| 258.5484451731355|5.631098924402049|0.9051152264048627|null| null|   null|74.83528402648182|7.873983847954116|\n",
      "|    min|                 0|                 1|              949|                25|                80|                2|                14|  no|   no|     no|               39|                1|\n",
      "|    max|              6258|              6259|             5399|               100|              2100|               32|                17| yes|  yes|    yes|              339|               35|\n",
      "+-------+------------------+------------------+-----------------+------------------+------------------+-----------------+------------------+----+-----+-------+-----------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# descriptive statistics\n",
    "machines_spark.describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualisation in spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[premium: string, avg(price): double]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[hd: bigint, price: bigint]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[ram: bigint, price: bigint]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[speed: bigint, price: bigint]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[screen: bigint, price: bigint]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[cd: string, avg(price): double]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[multi: string, avg(price): double]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# premium price\n",
    "from pyspark.sql.functions import avg\n",
    "pp = df.groupby(\"premium\").agg(avg(\"price\"))\n",
    "display(pp)\n",
    "\n",
    "# price\n",
    "display(df[['price']])\n",
    "\n",
    "# hd price\n",
    "from pyspark.sql.functions import avg\n",
    "hp = df[[\"hd\", \"price\"]]\n",
    "display(hp)\n",
    "\n",
    "# ram price\n",
    "rp = df[[\"ram\", \"price\"]]\n",
    "display(rp)\n",
    "\n",
    "# speed price\n",
    "sp = df[[\"speed\", \"price\"]]\n",
    "display(sp)\n",
    "\n",
    "# screen price\n",
    "sp = df[[\"screen\", \"price\"]]\n",
    "display(sp)\n",
    "\n",
    "# cd price\n",
    "cp = df.groupby(\"cd\").agg(avg(\"price\"))\n",
    "display(cp)\n",
    "\n",
    "# multi price\n",
    "mp = df.groupby(\"multi\").agg(avg(\"price\"))\n",
    "display(mp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hive intergration\n",
    "> To persist a Spark DataFrame into HDFS, where it can be queried using default Hadoop SQL engine (Hive), one straightforward strategy (not the only one) is to create a temporal view from that DataFrame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_hdfs.createOrReplaceTempView(\"df\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Once the temporal view is created, it can be used from Spark SQL engine to create a real table using create table as select. Before creating this table, I will create a new database called analytics to store it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hive sql queries\n",
    "# droping table\n",
    "sql_drop_table = \"\"\"\n",
    "drop table if exists analytics.pandas_spark_hive\n",
    "\"\"\"\n",
    "\n",
    "# droping database\n",
    "sql_drop_database = \"\"\"\n",
    "drop database if exists analytics cascade\n",
    "\"\"\"\n",
    "\n",
    "# creating database\n",
    "sql_create_database = \"\"\"\n",
    "create database if not exists analytics\n",
    "location '/user/cloudera/analytics/'\n",
    "\"\"\"\n",
    "\n",
    "# creating table\n",
    "sql_create_table = \"\"\"\n",
    "create table if not exists analytics.pandas_spark_hive\n",
    "using parquet\n",
    "as select price as price, *\n",
    "from machines_spark\n",
    "\"\"\"\n",
    "\n",
    "print(\"dropping database...\")\n",
    "result_drop_db = spark.sql(sql_drop_database)\n",
    "\n",
    "print(\"creating database...\")\n",
    "result_create_db = spark.sql(sql_create_database)\n",
    "\n",
    "print(\"dropping table...\")\n",
    "result_droptable = spark.sql(sql_drop_table)\n",
    "\n",
    "print(\"creating table...\")\n",
    "result_create_table = spark.sql(sql_create_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apache Arrow (PyArrow) and HDFS\n",
    "\n",
    "> Apache Arrow, is a in-memory columnar data format created to support high performance operations in Big Data environments (it can be seen as  the parquet format in-memory equivalent). It is developed in C++, but  its Python API is amazing as you will be able to see now, but first of all, please install it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting package metadata (current_repodata.json): done\n",
      "Solving environment: done\n",
      "\n",
      "## Package Plan ##\n",
      "\n",
      "  environment location: /home/ada/anaconda3\n",
      "\n",
      "  added / updated specs:\n",
      "    - pyarrow\n",
      "\n",
      "\n",
      "The following packages will be SUPERSEDED by a higher-priority channel:\n",
      "\n",
      "  conda              conda-forge::conda-4.9.2-py38h578d9bd~ --> pkgs/main::conda-4.9.2-py38h06a4308_0\n",
      "\n",
      "\n",
      "Preparing transaction: done\n",
      "Verifying transaction: done\n",
      "Executing transaction: done\n"
     ]
    }
   ],
   "source": [
    "!conda install pyarrow -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting package metadata (current_repodata.json): done\n",
      "Solving environment: failed with initial frozen solve. Retrying with flexible solve.\n",
      "Solving environment: failed with repodata from current_repodata.json, will retry with next repodata source.\n",
      "Collecting package metadata (repodata.json): done\n",
      "Solving environment: failed with initial frozen solve. Retrying with flexible solve.\n",
      "Solving environment: - \n",
      "Found conflicts! Looking for incompatible packages.\n",
      "This can take several minutes.  Press CTRL-C to abort.\n",
      "Examining conflict for python-libarchive-c pyyaml wurlitzer sphinxcontrib-jsmat| ^C\n",
      "                                                                               failed\n",
      "\n",
      "CondaError: KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!conda install libhdfs3 pyarrow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyarrow as pa\n",
    "import os\n",
    "os.environ['ARROW_LIBHDFS_DIR'] = '/opt/cloudera/parcels/CDH-5.14.4-1.cdh5.14.4.p0.3/lib64/'\n",
    "hdfs_interface = pa.hdfs.connect(host='localhost', port=8020, user='cloudera')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list files in hdfs\n",
    "hdfs_interface.ls('/user/cloudera/analytics/pandas_spark_hive/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading files directly from hdfs\n",
    "table = hdfs_interface.read_parquet('/user/cloudera/analytics/pandas_spark_hive/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# uploading local files to hdfs\n",
    "cwd = Path('./data/')\n",
    "destination_path = '/user/cloudera/analytics/data/'\n",
    "\n",
    "for f in cwd.rglob('*.*'):\n",
    "    print(f'uploading {f.name}')\n",
    "    with open(str(f), 'rb') as f_upl:\n",
    "        hdfs_interface.upload(destination_path + f.name, f_upl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### impyla: Hive + Impala SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting package metadata (current_repodata.json): done\n",
      "Solving environment: done\n",
      "\n",
      "## Package Plan ##\n",
      "\n",
      "  environment location: /home/ada/anaconda3\n",
      "\n",
      "  added / updated specs:\n",
      "    - impyla\n",
      "    - thrift_sasl\n",
      "\n",
      "\n",
      "The following packages will be downloaded:\n",
      "\n",
      "    package                    |            build\n",
      "    ---------------------------|-----------------\n",
      "    cyrus-sasl-2.1.27          |       hf484d3e_7         276 KB\n",
      "    impyla-0.16.3              |           py38_0         357 KB\n",
      "    libdb-6.1.26               |       he6710b0_0        17.7 MB\n",
      "    sasl-0.2.1                 |   py38h779454e_1          58 KB\n",
      "    thrift-0.13.0              |   py38he6710b0_0         120 KB\n",
      "    thrift_sasl-0.4.2          |           py38_1          10 KB\n",
      "    thriftpy-0.3.9             |   py38h7b6447c_2         199 KB\n",
      "    ------------------------------------------------------------\n",
      "                                           Total:        18.7 MB\n",
      "\n",
      "The following NEW packages will be INSTALLED:\n",
      "\n",
      "  cyrus-sasl         pkgs/main/linux-64::cyrus-sasl-2.1.27-hf484d3e_7\n",
      "  impyla             pkgs/main/linux-64::impyla-0.16.3-py38_0\n",
      "  libdb              pkgs/main/linux-64::libdb-6.1.26-he6710b0_0\n",
      "  sasl               pkgs/main/linux-64::sasl-0.2.1-py38h779454e_1\n",
      "  thrift             pkgs/main/linux-64::thrift-0.13.0-py38he6710b0_0\n",
      "  thrift_sasl        pkgs/main/linux-64::thrift_sasl-0.4.2-py38_1\n",
      "  thriftpy           pkgs/main/linux-64::thriftpy-0.3.9-py38h7b6447c_2\n",
      "\n",
      "\n",
      "\n",
      "Downloading and Extracting Packages\n",
      "libdb-6.1.26         | 17.7 MB   | ##################################### | 100% \n",
      "impyla-0.16.3        | 357 KB    | ##################################### | 100% \n",
      "cyrus-sasl-2.1.27    | 276 KB    | ##################################### | 100% \n",
      "thriftpy-0.3.9       | 199 KB    | ##################################### | 100% \n",
      "thrift_sasl-0.4.2    | 10 KB     | ##################################### | 100% \n",
      "thrift-0.13.0        | 120 KB    | ##################################### | 100% \n",
      "sasl-0.2.1           | 58 KB     | ##################################### | 100% \n",
      "Preparing transaction: done\n",
      "Verifying transaction: done\n",
      "Executing transaction: done\n"
     ]
    }
   ],
   "source": [
    "!conda install impyla thrift_sasl -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting thriftpy2\n",
      "  Downloading thriftpy2-0.4.12.tar.gz (356 kB)\n",
      "\u001b[K     |████████████████████████████████| 356 kB 58 kB/s eta 0:00:011\n",
      "\u001b[?25hRequirement already satisfied: ply<4.0,>=3.4 in ./anaconda3/lib/python3.8/site-packages (from thriftpy2) (3.11)\n",
      "Building wheels for collected packages: thriftpy2\n",
      "  Building wheel for thriftpy2 (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for thriftpy2: filename=thriftpy2-0.4.12-cp38-cp38-linux_x86_64.whl size=1171548 sha256=9ecb423b76170250ee6dcd73085a136635fb211d92406834b435ccc487bc2033\n",
      "  Stored in directory: /home/ada/.cache/pip/wheels/ad/a6/d0/c948df29931021b048a99d5ab2bc46d5f348b657342f23c075\n",
      "Successfully built thriftpy2\n",
      "Installing collected packages: thriftpy2\n",
      "Successfully installed thriftpy2-0.4.12\n"
     ]
    }
   ],
   "source": [
    "!pip install thriftpy2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from impala.dbapi import connect\n",
    "from impala.util import as_pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from hive to pandas\n",
    "hive_conn = connect(host='localhost', port=10000, database='analytics', auth_mechanism='PLAIN')\n",
    "\n",
    "with hive_conn.cursor() as c:\n",
    "    c.execute('SELECT * FROM analytics.pandas_spark_hive LIMIT 100')\n",
    "    results = c.fetchall()\n",
    "    \n",
    "with hive_conn.cursor() as c:\n",
    "    c.execute('SELECT * FROM analytics.pandas_spark_hive LIMIT 100')\n",
    "    results_df = as_pandas(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
